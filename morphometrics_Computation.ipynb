{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sally-Ama-Sampson/Estimating-Tair-from-CS-generated-Data-/blob/main/morphometrics_Computation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLxhxz1Lx8m0"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Program Name: Building morphological analysis\n",
        "# Purpose: Morphometrics computed on top of building footprint polygons for morphological typology analysis\n",
        "\n",
        "# This Python 3 environment mainly depends on Momepy, GeoPandas, PyGEOS, Scipy\n",
        "\n",
        "# Version:     0.1\n",
        "#              Functionalities:\n",
        "#              1. Pre-processing building footprints;\n",
        "#              2. Building morphometrics;\n",
        "#              ?. Morphological clustering will be implemented once multiple cities are processed.\n",
        "\n",
        "# Author:      Jiong (Jon) Wang, Angela Abascal, Stefanos Georganos, Martin Fleischmann\n",
        "#\n",
        "# Created:     02/05/2024\n",
        "# Copyright:   (c) J. Wang, A. Abascal, S. Georganos, M. Fleischmann, 2024\n",
        "# Licence:     MIT\n",
        "#-------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzTRzMx-x8m4"
      },
      "source": [
        "# Urban Morphology With Building Morphometrics\n",
        "\n",
        "There are two parts: (1) Preparation of urban elements for morphological analyis, and (2) Computation of morphometrics upon the prepared urban elements.\n",
        "\n",
        "The input data for now is completely from the Google AI Open Buildings (@ https://sites.research.google/open-buildings/), but other GIS vector data about basic urban elements, e.g. streets and districts, can be used as well. For now, the building data from Google is considered only with its 2D form, but the height as the 3rd dimension can be involved in the future.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUVEQ5z7x8m5"
      },
      "source": [
        "# Part I: Preparing Urban Elements For Morphometrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHXZ3DfQx8m6"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages/modules. Most of the packages/modules used are imported here, and only a few others are imported along the codes below.\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import momepy as mm\n",
        "import osmnx as ox\n",
        "import numpy as np\n",
        "import libpysal\n",
        "import scipy as sp\n",
        "import mapclassify\n",
        "import mapclassify.classifiers as classifiers\n",
        "import contextily as cx\n",
        "import utils\n",
        "\n",
        "from shapely.geometry import box\n",
        "from shapely import wkt\n",
        "from tqdm import tqdm\n",
        "from momepy import limit_range\n",
        "from inequality.theil import Theil\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBEJJ_usx8m6"
      },
      "source": [
        "### Load Raw Data from Google Open Buildings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQaPsxeFx8m6"
      },
      "outputs": [],
      "source": [
        "# Create Geopandas data frame from the *.csv downloaded from Google.\n",
        "# Assign proper coordinate system.\n",
        "\n",
        "google_building_file = 'harare.geojson'\n",
        "city_name = 'Harare'\n",
        "\n",
        "buildings = gpd.read_file(google_building_file)\n",
        "\n",
        "'''\n",
        "df = pd.read_csv(google_building_file)\n",
        "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
        "buildings = gpd.GeoDataFrame(df, crs='epsg:4326')\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVoAgY94x8m7"
      },
      "outputs": [],
      "source": [
        "# Check data formality by showing the first few lines of the dataframe table\n",
        "\n",
        "buildings.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1sKd59Vx8m7"
      },
      "outputs": [],
      "source": [
        "# Check metadata to see if coordinates and projections are correct.\n",
        "\n",
        "buildings.crs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMKVIzM2x8m7"
      },
      "outputs": [],
      "source": [
        "# Subset of columns as some columns are not necessary to keep for further processing.\n",
        "\n",
        "buildings = buildings[['geometry']]\n",
        "buildings.tail()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCUlQX_ux8m8"
      },
      "outputs": [],
      "source": [
        "# Load clip extent, to be defined.\n",
        "\n",
        "'''\n",
        "clip_extent = gpd.read_file('clip.shp').to_crs(4326)\n",
        "print(\"clip extent coordinate:\", clip_extent.crs)\n",
        "'''\n",
        "# All bounds are from the GHS Urban Centre database\n",
        "global_bound = gpd.read_file(\"ucdb2019.gpkg\")\n",
        "\n",
        "# Extract a city boundary from the dataframe\n",
        "bound = global_bound[global_bound.UC_NM_MN == city_name]\n",
        "\n",
        "# add centroid\n",
        "bound['centroid'] = bound['geometry'].centroid\n",
        "\n",
        "# Overview extent\n",
        "print(bound.crs)\n",
        "ax = bound.plot(figsize=(10, 5), color='lightgrey', alpha=0.5, edgecolor='red', linewidth=1.0, facecolor=None)\n",
        "cx.add_basemap(ax, source=cx.providers.CartoDB.Positron) #crs='EPSG:4326',\n",
        "ax.set_axis_off()\n",
        "\n",
        "# Show more detail of the extent\n",
        "bound\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYTzxcHax8m8"
      },
      "outputs": [],
      "source": [
        "# Clip buildings into extent\n",
        "\n",
        "# buildings = gpd.clip(buildings, bound)\n",
        "\n",
        "# Check the number of buildings within the extent\n",
        "print(buildings.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uw6pQidzx8m8"
      },
      "outputs": [],
      "source": [
        "# Check again by showing the clipped dataframe formality.\n",
        "\n",
        "buildings.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xq5BAdvx8m9"
      },
      "outputs": [],
      "source": [
        "# Check again the clipped building metadata\n",
        "\n",
        "buildings.crs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl_ENfOix8m9"
      },
      "source": [
        "### Preprocess buildings\n",
        "\n",
        "Mainly the Geometric correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPAqAUJfx8m9"
      },
      "outputs": [],
      "source": [
        "# Delete incomplete polygons\n",
        "\n",
        "buildings.geometry = buildings.buffer(0)\n",
        "buildings = buildings[~buildings.geometry.isna()]\n",
        "buildings = buildings.reset_index(drop=True).explode().reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HeLIZHIx8m9"
      },
      "outputs": [],
      "source": [
        "# Quick statistics about the total\n",
        "\n",
        "buildings.geom_type.value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i3J-PY-x8m9"
      },
      "outputs": [],
      "source": [
        "# Close holes\n",
        "\n",
        "buildings = gpd.GeoDataFrame(geometry=utils.fill_insides(buildings))\n",
        "buildings[\"uID\"] = range(len(buildings))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvGf0S2px8m9"
      },
      "outputs": [],
      "source": [
        "# Reproject building data into local UTM epsg\n",
        "\n",
        "def convert_wgs_to_utm(lon: float, lat: float):\n",
        "    \"\"\"Based on lat and lng, return best utm epsg-code\"\"\"\n",
        "    utm_band = str((math.floor((lon + 180) / 6 ) % 60) + 1)\n",
        "    if len(utm_band) == 1:\n",
        "        utm_band = '0'+utm_band\n",
        "    if lat >= 0:\n",
        "        epsg_code = '326' + utm_band\n",
        "        return epsg_code\n",
        "    epsg_code = '327' + utm_band\n",
        "    return epsg_code\n",
        "\n",
        "local_epsg = convert_wgs_to_utm(bound['centroid'].x.values[0], bound['centroid'].y.values[0])\n",
        "\n",
        "\n",
        "buildings = buildings.to_crs(local_epsg)\n",
        "buildings.crs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "8jFtKKQdx8m-"
      },
      "outputs": [],
      "source": [
        "# Save the preprocessed building data\n",
        "\n",
        "buildings.to_parquet('buildings.pq')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxRr8T29x8m-"
      },
      "source": [
        "### Generate tessellation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zkWgalZx8m-"
      },
      "outputs": [],
      "source": [
        "# Check building elements invalid for creating tessellation\n",
        "\n",
        "check = mm.CheckTessellationInput(buildings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nV1YjYIx8m-"
      },
      "outputs": [],
      "source": [
        "# Delete elements invalid for tessellation\n",
        "\n",
        "buildings = buildings.drop(check.collapse.index.union(check.overlap.index).union(check.split.index))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEEwEjoYx8m-"
      },
      "outputs": [],
      "source": [
        "# Operate tessellation\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "limit = mm.buffered_limit(buildings, 100)\n",
        "%time tess = mm.Tessellation(buildings, \"uID\", limit, segment=2).tessellation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqOnWw83x8m-"
      },
      "outputs": [],
      "source": [
        "# Save tessellation results\n",
        "\n",
        "tess.to_parquet('tessellation.pq')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai6d0CG7x8m-"
      },
      "outputs": [],
      "source": [
        "# Quick check tessellation total\n",
        "\n",
        "tess.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToMB7Wu3x8m-"
      },
      "outputs": [],
      "source": [
        "# Visualize\n",
        "\n",
        "buildings.plot(figsize=(15,15))\n",
        "tess.plot(figsize=(15,15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHIqvRBGx8m_"
      },
      "outputs": [],
      "source": [
        "# Simple check if all buildings are considered in tessellation\n",
        "\n",
        "if len(buildings) == len(tess):\n",
        "    print('The buildings along with the tessellation are prepared!')\n",
        "else:\n",
        "    print('!ATTENTION!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCC1sH5ex8m_"
      },
      "source": [
        "# Part II: Computing Morphometrics\n",
        "\n",
        "Including *primary* and *contextual* level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf7KF3Uex8m_"
      },
      "source": [
        "### Primary\n",
        "\n",
        "Primary morphometrics are computed at locations of each building and tessellation element.\n",
        "\n",
        "This is independent from Part I, but please do import necessary packages/modules by running the first block in Part I."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xzqcbDGx8m_"
      },
      "outputs": [],
      "source": [
        "# Read saved building and tessellation\n",
        "\n",
        "blg = gpd.read_parquet('buildings.pq')\n",
        "tess = gpd.read_parquet('tessellation.pq')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sbOBW3Yx8m_"
      },
      "outputs": [],
      "source": [
        "# Simple building morphometrics\n",
        "\n",
        "blg['sdbAre'] = mm.Area(blg).series\n",
        "blg['sdbPer'] = mm.Perimeter(blg).series\n",
        "blg['ssbCCo'] = mm.CircularCompactness(blg, 'sdbAre').series\n",
        "blg['ssbCor'] = mm.Corners(blg).series\n",
        "blg['ssbSqu'] = mm.Squareness(blg).series\n",
        "blg['ssbERI'] = mm.EquivalentRectangularIndex(blg, 'sdbAre', 'sdbPer').series\n",
        "blg['ssbElo'] = mm.Elongation(blg).series\n",
        "\n",
        "cencon = mm.CentroidCorners(blg)\n",
        "blg['ssbCCM'] = cencon.mean\n",
        "blg['ssbCCD'] = cencon.std\n",
        "\n",
        "blg['stbOri'] = mm.Orientation(blg).series\n",
        "tess['stcOri'] = mm.Orientation(tess).series\n",
        "blg['stbCeA'] = mm.CellAlignment(blg, tess, 'stbOri', 'stcOri', 'uID', 'uID').series\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2AW1wfOx8m_"
      },
      "outputs": [],
      "source": [
        "# Simple tessellation morphometrics\n",
        "\n",
        "tess['sdcLAL'] = mm.LongestAxisLength(tess).series\n",
        "tess['sdcAre'] = mm.Area(tess).series\n",
        "tess['sscCCo'] = mm.CircularCompactness(tess, 'sdcAre').series\n",
        "tess['sscERI'] = mm.EquivalentRectangularIndex(tess, 'sdcAre').series\n",
        "tess['sicCAR'] = mm.AreaRatio(tess, blg, 'sdcAre', 'sdbAre', 'uID').series\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMNdL7Bax8m_"
      },
      "outputs": [],
      "source": [
        "# Morphometrics considering spatial neighborhood characteristics\n",
        "\n",
        "queen_1 = libpysal.weights.contiguity.Queen.from_dataframe(tess, ids=\"uID\", silence_warnings=True)\n",
        "\n",
        "blg[\"mtbAli\"] = mm.Alignment(blg, queen_1, \"uID\", \"stbOri\").series\n",
        "blg[\"mtbNDi\"] = mm.NeighborDistance(blg, queen_1, \"uID\").series\n",
        "tess[\"mtcWNe\"] = mm.Neighbors(tess, queen_1, \"uID\", weighted=True).series\n",
        "tess[\"mdcAre\"] = mm.CoveredArea(tess, queen_1, \"uID\").series\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTGNWKr7x8nA"
      },
      "outputs": [],
      "source": [
        "# Save intermediate results\n",
        "\n",
        "tess.drop(columns='geometry').to_parquet('tess_data.parquet')\n",
        "blg.drop(columns='geometry').to_parquet('blg_data.parquet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1mLbBNTx8nA"
      },
      "outputs": [],
      "source": [
        "# Morphometrics considering wider spatial neighborhood characters\n",
        "\n",
        "queen_3 = mm.sw_high(k=3, weights=queen_1)\n",
        "\n",
        "blg_q1 = libpysal.weights.contiguity.Queen.from_dataframe(blg, silence_warnings=True)\n",
        "\n",
        "blg['ltbIBD'] = mm.MeanInterbuildingDistance(blg, queen_1, 'uID', queen_3, verbose=False).series\n",
        "blg['ltcBuA'] = mm.BuildingAdjacency(blg, queen_3, 'uID', blg_q1, verbose=False).series\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXyBk-Mix8nA"
      },
      "outputs": [],
      "source": [
        "# Update unique ID\n",
        "\n",
        "tess = tess.merge(blg[['uID']], on='uID', how='left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9Ghm5yfx8nA"
      },
      "outputs": [],
      "source": [
        "# Save again some intermediate results\n",
        "\n",
        "tess.drop(columns='geometry').to_parquet('tess_data.parquet')\n",
        "blg.drop(columns='geometry').to_parquet('blg_data.parquet')\n",
        "\n",
        "fo = libpysal.io.open('queen1.gal', 'w')\n",
        "fo.write(queen_1)\n",
        "fo.close()\n",
        "\n",
        "fo = libpysal.io.open('queen3.gal', 'w')\n",
        "fo.write(queen_3)\n",
        "fo.close()\n",
        "\n",
        "fo = libpysal.io.open('blg_queen.gal', 'w')\n",
        "fo.write(blg_q1)\n",
        "fo.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3W1_Meix8nA"
      },
      "outputs": [],
      "source": [
        "# Join results\n",
        "\n",
        "merged = tess.merge(blg.drop(columns=['geometry']), on='uID')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuprXYlIx8nA"
      },
      "outputs": [],
      "source": [
        "# Save results for primary morphometrics\n",
        "\n",
        "primary = merged.drop(columns=['geometry'])\n",
        "primary.to_parquet('primary.parquet')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9nd4w7ox8nF"
      },
      "source": [
        "## Contextual\n",
        "\n",
        "Contextual morphometrics are statistics of the primary morphometrics in a local spatial extent around each element of building and tesselation, hence, pure statistics to bring in more spatial variations, as opposed to new morphometrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv2CaSMvx8nF"
      },
      "outputs": [],
      "source": [
        "# Load saved morphometrics\n",
        "\n",
        "primary = pd.read_parquet('primary.parquet')\n",
        "geom = gpd.read_parquet('tessellation.pq', columns=[\"geometry\"])\n",
        "\n",
        "gdf = primary.set_index('uID')\n",
        "\n",
        "spatial_weights = queen_3\n",
        "\n",
        "unique_id = 'uID'\n",
        "\n",
        "gdf = gdf.replace(np.inf, np.nan).fillna(0)  # normally does not happen, but to be sure\n",
        "chars = gdf.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pbsC7f0x8nF"
      },
      "outputs": [],
      "source": [
        "# Check computed primary morphometrics by listing the names\n",
        "\n",
        "chars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eabihjNNx8nF"
      },
      "outputs": [],
      "source": [
        "# Define function for statistics\n",
        "\n",
        "def theil(y):\n",
        "    y = np.array(y)\n",
        "    n = len(y)\n",
        "    plus = y + np.finfo('float').tiny * (y == 0)  # can't have 0 values\n",
        "    yt = plus.sum(axis=0)\n",
        "    s = plus / (yt * 1.0)\n",
        "    lns = np.log(n * s)\n",
        "    slns = s * lns\n",
        "    t = sum(slns)\n",
        "    return t\n",
        "\n",
        "def _simpson_di(data):\n",
        "\n",
        "    def p(n, N):\n",
        "        if n == 0:\n",
        "            return 0\n",
        "        return float(n) / N\n",
        "\n",
        "    N = sum(data.values())\n",
        "\n",
        "    return sum(p(n, N) ** 2 for n in data.values() if n != 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bLmgne4x8nG"
      },
      "outputs": [],
      "source": [
        "# Call functions\n",
        "\n",
        "skewness = pd.DataFrame(index=chars)\n",
        "for c in chars:\n",
        "    skewness.loc[c, 'skewness'] = sp.stats.skew(gdf[c])\n",
        "headtail = list(skewness.loc[skewness.skewness >= 1].index)\n",
        "to_invert = skewness.loc[skewness.skewness <= -1].index\n",
        "\n",
        "for inv in to_invert:\n",
        "    gdf[inv + '_r'] = gdf[inv].max() - gdf[inv]\n",
        "inverted = [x for x in gdf.columns if '_r' in x]\n",
        "headtail = headtail + inverted\n",
        "natural = [x for x in chars if x not in headtail]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzWhj9R2x8nG"
      },
      "outputs": [],
      "source": [
        "# Create placeholders to save major stats\n",
        "\n",
        "bins = {}\n",
        "for c in headtail:\n",
        "    bins[c] = mapclassify.HeadTailBreaks(gdf[c]).bins\n",
        "for c in natural:\n",
        "    bins[c] = mapclassify.gadf(gdf[c], method='NaturalBreaks')[1].bins\n",
        "\n",
        "means = {}\n",
        "ranges = {}\n",
        "theils = {}\n",
        "simpsons = {}\n",
        "\n",
        "for ch in gdf.columns:\n",
        "    means[ch] = []\n",
        "    ranges[ch] = []\n",
        "    theils[ch] = []\n",
        "    simpsons[ch] = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXHzQn7Xx8nG"
      },
      "outputs": [],
      "source": [
        "# Save major stats\n",
        "\n",
        "for index, _ in tqdm(gdf.iterrows(), total=gdf.shape[0]):\n",
        "    #print(index)\n",
        "    neighbours = [index]\n",
        "    neighbours += spatial_weights.neighbors[index]\n",
        "\n",
        "    subset = gdf.loc[neighbours]\n",
        "    for ch in chars:\n",
        "        values_list = subset[ch]\n",
        "        idec = limit_range(values_list, rng=(10, 90))\n",
        "        iquar = limit_range(values_list, rng=(25, 75))\n",
        "\n",
        "        means[ch].append(np.mean(iquar))\n",
        "        ranges[ch].append(max(iquar) - min(iquar))\n",
        "        theils[ch].append(theil(idec))\n",
        "\n",
        "        sample_bins = classifiers.UserDefined(values_list, list(bins[ch]))\n",
        "        counts = dict(zip(bins[ch], sample_bins.counts))\n",
        "        simpsons[ch].append(_simpson_di(counts))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03LZ8lZ0x8nG"
      },
      "outputs": [],
      "source": [
        "# Attach to the original dataframe table in one piece\n",
        "\n",
        "contextual = {}\n",
        "for ch in chars:\n",
        "    print(ch)\n",
        "    contextual[ch + '_meanIQ3'] = means[ch]\n",
        "    contextual[ch + '_rangeIQ3'] = ranges[ch]\n",
        "    contextual[ch + '_theilID3'] = theils[ch]\n",
        "    contextual[ch + '_simpson'] = simpsons[ch]\n",
        "\n",
        "contextual = pd.DataFrame(contextual, index=gdf.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaNZ_faUx8nG"
      },
      "outputs": [],
      "source": [
        "# Save weights\n",
        "\n",
        "#import scipy\n",
        "import pickle\n",
        "\n",
        "#scipy.sparse.save_npz(\"w10.npz\", spatial_weights.sparse)\n",
        "with open('spatial_weights.pickle', 'wb') as handle:\n",
        "    pickle.dump(spatial_weights, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPV3URJHx8nG"
      },
      "outputs": [],
      "source": [
        "# Save the main contextual results\n",
        "\n",
        "contextual.to_parquet('contextual.parquet')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}